{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not a built-in stop list: English",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cf36a86a2801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[0mTfidfVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLemNormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"English\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \"\"\"\n\u001b[0;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m         \u001b[0manalyze\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    963\u001b[0m         \u001b[0mj_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mbuild_analyzer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'word'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m             \u001b[0mtokenize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             self._check_stop_words_consistency(stop_words, preprocess,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_stop_words\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;34m\"\"\"Build or fetch the effective stop words list\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_check_stop_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_stop_words_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_stop_list\u001b[1;34m(stop)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"not a built-in stop list: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not a built-in stop list: English"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "f = open(\"Otro.txt\", \"r\", errors=\"ignore\")\n",
    "raw = f.read()\n",
    "\n",
    "raw = raw.lower()\n",
    "\n",
    "# convierte a una lista de sentencias\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "word_tokens = nltk.word_tokenize(raw)  # lo convierte en una lista de palabras\n",
    "\n",
    "sent_tokens[:2]\n",
    "['a chatbot (also known as a talkbot, chatterbot, bot, im bot, interactive agent, or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
    " 'such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.']\n",
    "word_tokens[:2]\n",
    "['a', 'chatbot', '(', 'also', 'known']\n",
    "\n",
    "# Se toman como entrada los tokens y devolver√° tokens normalizados.\n",
    "\n",
    "lammer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return[lammer.lemmatize(token) for token in tokens]\n",
    "remove_punch_dict = ((ord(punct), None)for punct in string.punctuation)\n",
    "\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().traslate(remove_punch_dict)))\n",
    "\n",
    "\n",
    "# Funci√≥n de saludo a trav√©s el bot.\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\", \"hola\",\"Que onda\", \"Que tal\", \"Que pex\", \"Que show\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\",\n",
    "                      \"hello\", \"I am glad! You are talking to me\", \"Hola\", \"Que tal\", \"Que hongo\", \"Que hay\", \"Holo\"]\n",
    "\n",
    "\n",
    "def greeting(sentence):\n",
    "    for palabra in sentence.split():\n",
    "        if palabra.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Funci√≥n de Respuesta \n",
    "def response(user_response):\n",
    "    robo_response = \"\"\n",
    "    sent_tokens.append(user_response)\n",
    "\n",
    "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"English\")\n",
    "tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "idx = vals.argsort()[0][-2]\n",
    "flat = vals.flatten()\n",
    "flat.sort()\n",
    "req_tfidf = flat[-2]\n",
    "\n",
    "if(req_tfidf == 0):\n",
    "    robo_response = robo_response + \"mmm no te endiendo\"\n",
    "    print(robo_response)\n",
    "\n",
    "else:\n",
    "    robo_response = robo_response+sent_tokens[idx]\n",
    "    print(robo_response)\n",
    "\n",
    "# Finalmente, alimentaremos las\n",
    "# l√≠neas que queremos que diga nuestro robot al iniciar y finalizar una conversaci√≥n, seg√∫n la informaci√≥n del usuario.\n",
    "\n",
    "flag = True\n",
    "print(\"Hola, soy el chat bot type Bye!\")\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != \"adios\"):\n",
    "        if(user_response == \"gracias\" or user_response == \"Que buena onda\"):\n",
    "            flag = False\n",
    "            print(\"De nada\")\n",
    "        else:\n",
    "            if(greeting(user_response) != None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \", end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Bye! take care..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mover el numero \n",
      "None\n",
      "4-5-1\n",
      "8-3-7\n",
      "0-6-2\n",
      "movimiento: 1\n",
      "Mover el numero \n",
      "6\n",
      "4-5-1\n",
      "8-3-7\n",
      "6-0-2\n",
      "movimiento: 2\n",
      "Mover el numero \n",
      "3\n",
      "4-5-1\n",
      "8-0-7\n",
      "6-3-2\n",
      "movimiento: 3\n",
      "Mover el numero \n",
      "7\n",
      "4-5-1\n",
      "8-7-0\n",
      "6-3-2\n",
      "movimiento: 4\n",
      "Mover el numero \n",
      "2\n",
      "4-5-1\n",
      "8-7-2\n",
      "6-3-0\n",
      "movimiento: 5\n",
      "Mover el numero \n",
      "3\n",
      "4-5-1\n",
      "8-7-2\n",
      "6-0-3\n",
      "movimiento: 6\n",
      "Mover el numero \n",
      "6\n",
      "4-5-1\n",
      "8-7-2\n",
      "0-6-3\n",
      "movimiento: 7\n",
      "Mover el numero \n",
      "8\n",
      "4-5-1\n",
      "0-7-2\n",
      "8-6-3\n",
      "movimiento: 8\n",
      "Mover el numero \n",
      "7\n",
      "4-5-1\n",
      "7-0-2\n",
      "8-6-3\n",
      "movimiento: 9\n",
      "Mover el numero \n",
      "5\n",
      "4-0-1\n",
      "7-5-2\n",
      "8-6-3\n",
      "movimiento: 10\n",
      "Mover el numero \n",
      "1\n",
      "4-1-0\n",
      "7-5-2\n",
      "8-6-3\n",
      "movimiento: 11\n",
      "Mover el numero \n",
      "2\n",
      "4-1-2\n",
      "7-5-0\n",
      "8-6-3\n",
      "movimiento: 12\n",
      "Mover el numero \n",
      "3\n",
      "4-1-2\n",
      "7-5-3\n",
      "8-6-0\n",
      "movimiento: 13\n",
      "Mover el numero \n",
      "6\n",
      "4-1-2\n",
      "7-5-3\n",
      "8-0-6\n",
      "movimiento: 14\n",
      "Mover el numero \n",
      "8\n",
      "4-1-2\n",
      "7-5-3\n",
      "0-8-6\n",
      "movimiento: 15\n",
      "Mover el numero \n",
      "7\n",
      "4-1-2\n",
      "0-5-3\n",
      "7-8-6\n",
      "movimiento: 16\n",
      "Mover el numero \n",
      "4\n",
      "0-1-2\n",
      "4-5-3\n",
      "7-8-6\n",
      "movimiento: 17\n",
      "Mover el numero \n",
      "1\n",
      "1-0-2\n",
      "4-5-3\n",
      "7-8-6\n",
      "movimiento: 18\n",
      "Mover el numero \n",
      "2\n",
      "1-2-0\n",
      "4-5-3\n",
      "7-8-6\n",
      "movimiento: 19\n",
      "Mover el numero \n",
      "3\n",
      "1-2-3\n",
      "4-5-0\n",
      "7-8-6\n",
      "movimiento: 20\n",
      "Mover el numero \n",
      "6\n",
      "1-2-3\n",
      "4-5-6\n",
      "7-8-0\n",
      "movimiento: 21\n"
     ]
    }
   ],
   "source": [
    "import simpleai\n",
    "from simpleai.search import astar, SearchProblem\n",
    "\n",
    "#Busqueda en Profundidad\n",
    "COUNT = 0\n",
    "\n",
    "GOAL = '''1-2-3\n",
    "4-5-6\n",
    "7-8-0'''\n",
    "\n",
    "INITIAL = '''4-5-1\n",
    "8-3-7\n",
    "0-6-2'''\n",
    "\n",
    "\n",
    "def list_to_string(list_):\n",
    "    return '\\n'.join(['-'.join(row) for row in list_])\n",
    "\n",
    "\n",
    "def string_to_list(string_):\n",
    "    return [row.split('-') for row in string_.split('\\n')]\n",
    "\n",
    "\n",
    "def find_location(rows, element_to_find):\n",
    "    '''Encuentra la direccion de la ficha buscada, devuelve la tupla fila, columna'''\n",
    "    for ir, row in enumerate(rows):\n",
    "        for ic, element in enumerate(row):\n",
    "            if element == element_to_find:\n",
    "                return ir, ic\n",
    "\n",
    "\n",
    "#Se guarda la posicion fnal de cada pieza, para no tener que recalcular en cada ciclo\n",
    "goal_positions = {}\n",
    "rows_goal = string_to_list(GOAL)\n",
    "for number in '123456780':\n",
    "    goal_positions[number] = find_location(rows_goal, number)\n",
    "\n",
    "\n",
    "class EigthPuzzleProblem(SearchProblem):\n",
    "    def actions(self, state):\n",
    "        '''Retorna una lista de las piezas que podemso mover a un espacio vacio'''\n",
    "        rows = string_to_list(state)\n",
    "        row_e, col_e = find_location(rows, '0')\n",
    "\n",
    "        #COUNT + 1\n",
    "        \n",
    "        actions = []\n",
    "        if row_e > 0:\n",
    "            actions.append(rows[row_e - 1][col_e])\n",
    "        if row_e < 2:\n",
    "            actions.append(rows[row_e + 1][col_e])\n",
    "        if col_e > 0:\n",
    "            actions.append(rows[row_e][col_e - 1])\n",
    "        if col_e < 2:\n",
    "            actions.append(rows[row_e][col_e + 1])\n",
    "\n",
    "        #print('movimiento:'+str(COUNT))\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def result(self, state, action):\n",
    "        '''retorna el estado resultante despues de mover una ficha a un espacio vacio.\n",
    "           (La accion parameter contiene la pieza a mover)\n",
    "        '''\n",
    "        rows = string_to_list(state)\n",
    "        row_e, col_e = find_location(rows, '0')\n",
    "        row_n, col_n = find_location(rows, action)\n",
    "\n",
    "        rows[row_e][col_e], rows[row_n][col_n] = rows[row_n][col_n], rows[row_e][col_e]\n",
    "\n",
    "        return list_to_string(rows)\n",
    "\n",
    "    def is_goal(self, state):\n",
    "        '''Devuelve true si el estado actual es el estado deseado'''\n",
    "        return state == GOAL\n",
    "     \n",
    "    def heuristic(self, state):\n",
    "        '''Retorna una estimacion de la distancia al estado deseado\n",
    "           Usando la distancia manhatan \n",
    "        '''\n",
    "        rows = string_to_list(state)\n",
    "\n",
    "        distance = 0\n",
    "\n",
    "        for number in '123456780':\n",
    "            row_n, col_n = find_location(rows, number)\n",
    "            row_n_goal, col_n_goal = goal_positions[number]\n",
    "\n",
    "            distance += abs(row_n - row_n_goal) + abs(col_n - col_n_goal)\n",
    "\n",
    "        return distance\n",
    "\n",
    "\n",
    "result = astar(EigthPuzzleProblem(INITIAL))\n",
    "\n",
    "\n",
    "for action, state in result.path():\n",
    "    print ('Mover el numero ')\n",
    "    print (action)\n",
    "    print (state)\n",
    "    COUNT = COUNT + 1\n",
    "    print ('movimiento: '+str(COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
